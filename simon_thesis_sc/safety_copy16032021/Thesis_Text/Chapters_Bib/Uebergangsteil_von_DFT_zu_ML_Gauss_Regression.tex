\chapter{Uebergangsteil von DFT zu ML Gauss Regression}

\section{Motivation, warum ML}
\subsection{Pfad von der DFT zu ML Methode}
Die Ermitttelung von Energien atomarer Systeme mittels Dichtefunktionaltheorie bringt eine Steigerung gegenüber der exakten Lösung der Schrödingergleichung. Im Allgemeinen Fall, sind quantenmechanische Systeme allerdings so komplex, dass auch hier der Rechenaufwand den realistisch Möglichen übersteigt. Weiters beschreiben die Simulationen durch die DFT ein genaues Setting und sind durch ihre empirisch berechneten Funktionale limitiert. 
Die Motivation \textit{Machine-Learning} Methoden einzusetzen ist die genauen Beschreibung einer \textit{Born-Oppenheimer Fläche der potentiellen Energie (PES)}. 

Bisher ist das Konzept von \textit{PES} in Verwendung, um Energieflächen einer fixierten Atom-Konstellation zu modellieren. Mittels Machine-Learning Algorithmen wird versucht eine klare Energiefläche zu finden, welche die Energien einzelner Atome oder Atom-Bindungen, als Funktion ihrer Umgebung (\textit{neighbourhood-environment}) beschreibt. Ein solches System reagiert empfindlich auf Änderungen seiner Umgebung, wie zum Beispiel eine Veränderung der Anzahl, oder des Typs, der Umgebungs-Atome. \footcite[1051]{GAP-intro}

Jeder, und auch die hier besprochenen Machine-Learning-Algorithmen, sind nicht per se in der Lage atomare Systeme zu Lösen und Energien zu berechnen. Sie beruhen auf Trainingsdaten und Testdaten und benötigen daher eine vorausgehende quantenmechanische Methode um Daten-Sets zu generieren. Im Allgemeinen gilt, dass ML- und insbesondere die hier verwendeten Gauß-Regressions-Modelle, sehr gut  für Interpolation geeignet sind, nicht jedoch für Extrapolation. Das heißt, das trainierte \textit{Machine-Learning}-Modell funktioniert gut in den Bereichen der Trainingsdaten, nicht jedoch für exotischere Atom-Konstellationen, die nicht trainiert wurden.

\subsection{Was wird genau verwendet}
In dieser Arbeit wird zur Energieberechnung von atomaren Systemen ein, sich stochastischen Prozessen bedienendes, \textit{Framework} verwendet. Eine andere Möglichkeit wäre die Verwundung einfacher \textit{Neuronaler Netzwerke (NN)}. Es zeigt sich, dass ein geeignetes stochastisches Werkzeug der Gauß´sche Regressions-Prozess ist.

Um die mittel Regression Vorhersagen treffen zu können wird die atomare Energie eines Sets von Atomen, als Funktion ihrer Atom-Konfiguration, das heißt ihrer Geometrie eingeführt. Diese Energie besteht aus der Summe über alle Energiefunktionale innerhalb des  "Cut-Off-Radius" und einem vernachlässigbaren (long-range-) Term. Die Beiträge des letzten Terms beschreiben elektrostatische Kräfte, beispielsweise Polarisation oder Van-der-Waals Kräfte. 

$$TODO: E=DOPPELSUMME, wie in GAP-Intro, Seite 1051$$

Die Summe über $\alpha$ erstreckt sich über die verschiedenen Typen an Energiefunktionalen. Das sind im einfachsten Fall Zwei- oder Drei-Körper Energien, die sich durch atomare Abstände, oder Winkel zwischen den Atomen beschreiben lassen. Die Argumente dieser Energiefunktionale nennt man Deskriptoren. Deskriptoren beschreiben die Atomkonfiguration, mittels Transformation der kartesischen Koordinaten der Atome in Umgebungskoordinaten für die gesuchte lokale Energie eines Ziel-Atoms. Die Summe über $i$ läuft über alle Instanzen von Bindungs-Winkeln respektive Bindungs-Abständen. \footcite[1051]{GAP-intro}


\section{Die Gauß Regression}

\subsection{Vorraussetzungen für Gaussian Process}

Der Gauß´sche Prozess kann zur Lösung von Regressionsproblemen eingesetzt werden. Dazu nimmt man ein fixiertes Set von Basisfunktionen \textbf{H} an, in deren Raum der Dateninput \textbf{X} lebt. Die Dimension von $X$ sei \textit{N} und die Matrix \textbf{R} definiert den Input $TODO:x hoch (n)$ in der Basis \textbf{H}. Für die Wahl der Basis ergeben sich je nach Problemstellung unterschiedliche Optionen. Einfach Wahlen wären beispielsweise radiale oder polynomiale Basisfunktionen. Umgelegt auf die Auffindung von quantenmechanischen Energien, wird das Input-Datenset von Deskriptoren $\textbf{d}_N$ beschrieben. Die Deskriptoren stehen für die betrachtete atomare Umgebung und sollen klare Vorhersagen über die zugehörigen Energien machen. 

$$TODO: Formel 45.17 aus GAP-Info theory$$

Die vorhergesagten Werte werden mit $\textbf{y}_N$ angenommen und sind auch von Dimension \textit{N}. Der Input-Datensatz \textbf{X} in der Basis \textbf{H} wird durch einen Gewichtsvektor \textbf{w} auf die Ergebnisse $\textbf{y}_N$ \textit{gemapped}. Der Vektor \textbf{w} hat Rang \textit{N}.

$$TODO: Formel 45.18 aus GAP-Info theory$$

Mit \textit{David J.C. MacKay} nimmt man die Wahrscheinlichkeitsverteilung der vorhergesagten Werte $y$ als Normalverteilung an: 

$$TODO: Formel 45.19 aus GAP-Info theory$$

und die hierbei verwendete Kovarianzmatrix von \textbf{y} ist folgendermaßen definiert: \footcite[540]{GAP-info-theory}


$$TODO: Formel 45.20 aus GAP-Info theory$$

Die Hauptannahme hier ist, dass die Verteilung der Gewichte als Gauß´sche Normalverteilung angenommen wird. Dadurch fällt die, im Normalfall schwierige, \textit{fitting}-Aufgabe,  dass auffinden der Gewichte weg und wird mithilfe der Varianz $TODO: sigma hoch 2 und index w$ modelliert. Im Weiteren wird somit die Kovarianzmatrix, auch \textit{Kernel} genannt, das tragende Prognose-Instrument. \footcite[1052]{GAP-intro}




\subsection{Bauen der Covarianz Matrix, also trainieren des Modells}


Jedes Datenset \textbf{X} in einer gewählten Basis \textbf{H} induziert also ein Kovarianzmatrix, und nur dieses \textit{Kernel} wird für weitere Vorhersagen benötigt. Das Modell muss noch \textit{trainiert} werden. Dazu benötigt man \textit{target values} \textbf{t}. In dieser Arbeit sind diese Werte die Energieniveaus der, mit den zugehörigen Deskriptoren berechneten,  quantenmechanischen Systeme. Die Energien werden aus der Dichtefunktionaltheorie berechnet. Die Dimension von \textbf{t} ist \textit{N}. Das Regressionsmodell nimmt also ein Set an Deskriptoren $TODO: R(d_n) $und trainiert es mit den entsprechenden Energiewerten $\textbf{t}_N$. \footcite[3]{GAP-2009}

Aufgrund der Gauß´schen Modellierung, muss angenommen werden, dass jedes der \textit{target values} \textbf{t} auch um ein Gauß´sches vom vorhergesagten Wert \textbf{y} abweicht. Das führt dazu, dass \textbf{t} nicht scharf definiert ist, sondern auch einer Normalverteilung unterliegt. 


$$TODO: Formel 45.23 aus GAP-Info theory$$


Das führt zu einer \textit{trainierten} Kovarianzmatrix mit vollem Rang: \footcite[540]{GAP-info-theory}


$$TODO: Formel 45.24 aus GAP-Info theory$$

Anzumerken ist, dass die Wahl der Kovarianzmatrix, sprich des \textit{Kernels}, im Allgemeinen nicht vorbestimmt ist. Die einzige Einschränkung ist, dass die Matrix nicht negativ definit werden darf. Das muss für jedes Set  $\textbf{X}_n$an Datenpunkten gelten. 





\subsection{Vorraussagen machen mit Modell}

Die eigentliche Aufgabe des Modells ist es Vorhersagen zu machen, nachdem es mit dem Datenset, bestehend aus $\textbf{x}_N$ und $\textbf{t}_N$ trainiert wurde. Der \textit{N+1} wert für \textbf{t} ist gesucht, also $t_(N+1)$. 

Die Kovarinazmatrix \textbf{C} wird also um einen Eintrag erweitert und ist nun in Dimension \textit{N+1}. Zur einfacheren Darstellung, definiert man $\textbf{C}_(N+1)$ neu: \footcite[543]{GAP-info-theory}


$$TODO: Formel 45.35 aus GAP-Info theory$$


Die Wahrscheinlichkeitsverteilung, unter Berücksichtigung eines neuen, unbekannten Wertes, ist Gauß-verteilt und benutzt als \textit{Kernel} $\textbf{C}_(N+1)$: 

$$TODO: Formel 45.36 aus GAP-Info theory$$


Als vorhergesagten Wert $\textit{t}_(N+1)$ nimmt man nun den Mittelwert dieser Verteilung an: 


$$TODO: Formel 45.42 aus GAP-Info theory$$

Hier zeigt sich die die Einfachheit des Gauß´schen Modellierungs Prozesses. Zur Prognose neuer Energiewerte, wird nicht die Berechnung der Matrix $\textbf{C}_(N+1)$ benötigt, sondern nur die Invertierung der Matrix $\textbf{C}_N$. Somit sind einzig die, durch die Basis mit den Deskriptoren $\textbf{d}_N$ induzierte, Kovarianzmatrix  $\textbf{C}_N$ und die Trainings-Energieniveaus $\textbf{t}_N$ notwendig, um Interpolation durchzuführen.  \footcite[1052]{GAP-intro}





\section{Modifikationen zur quantenmechanischen Anwendung}


\subsection{Totale Energien und Kräfte}

Die Dichtefunktionaltheorie, sowie auch andere quantenmechanische Berechnungsmethoden, ermitteln Energien kompletter Systeme für eine gegebene Konfiguration. Der erklärte Gauß´sche Approximations Prozess nimmt lokale atomare Energien und Atom-Geometrien als Argumente. Lokale Energien und lokale Gradienten werden aus den quantenmechanisch berechneten Totalen Energien und ihre Totalen Gradienten abgeleitet. Die Berechnung der Kovarianz erfolgt analog zu der für lokale Energien mit zusätzlicher Summation über alle Atome des Systems. Die Analogie zur lokalen Kovarinaz-Matrix ist gegeben. \footcite[1053]{GAP-intro}


$$TODO: Formel 12 aus GAP-Intro, bisserl kürzer und paar Schritte auslassen $$

Die Kraft eines Systems ist durch die Ableitung nach dem Ort gegeben. Die Kraft im Bezug auf die Totale Energie ist gegeben als Gradient nach allen Atomen $k$ nach allen kartesischen Koordinaten $\alpha$. 

$$TODO: Formel 13 aus GAP-Intro, bisserl andere Notation$$

Die Gradienten werden nach den Deskriptoren gebildet und sind je nach Wahl der Deskriptoren zu berechnen. Im Falle der Ableitungen der Kovarianzmatrix ergibt sich eine Jacobi-Matrix durch Ableitung nach Koordinaten in beiden Systemen. \footcite[1053]{GAP-intro}


$$TODO: Formel 17 aus GAP-Intro, bisserl andere Notation$$

$$TODO: Wobei i element aus N und j element aus M, umschreiben in mathematische Schreibweise$$






\subsection{Beschränkung auf nahe Umgebung - cutoff radius}

Die Berechnung lokaler Energien beschränkt sich auf eine gewisse Umgebung der betrachteten Atome. Die Größe der Umgebung ist nicht eindeutig definiert und wird durch die gewünschte Genauigkeit der Ergebnisse induziert. Wird ein großes Umfeld mit einbezogen steigt der Rechenaufwand stark an, da mit der Göße des Umfeld die Anzahl der betrachteten Atome steigt. Hingegen liefert eine zu klein gewählte Umgebung ungenaue und nicht repräsentative Ergebnisse. 

Ein, die Umgebung begrenzender, Radius muss gewählt. Die lokale Energie geht fließend Richtung Null, sobald der radiale Abstand des betrachteten Atoms den Begrenzungs-Radius überschreitet. Eine einfach Veranschaulichung für eine beliebige lokale Energie: \footcite[1053]{GAP-intro}\footcite[2]{GAP-2009}


$$TODO: Formel 22 aus GAP-Intro$$

Der Begrenzungs-Radius hat die Form: \footcite[1054]{GAP-intro}


$$TODO: Formel 22 aus GAP-Intro, nur als mittlere Funktion eine allgemeine Funktion von r, r_cut und d angeben. r_cut = r_begrenzung$$

Hier ist $TODO: allg. Funktion von (r,r_begr, d)$ eine Funktion die von r, rbegr und d abhängt, mit d als veränderlicher Parameter. Der Parameter $d$ determiniert die Göße der Übergangsregion in der Nähe des Cutoffradius. Die Parameter $d$ und $r_begr$ können bei einer möglichen Applikation vom Anwender definiert werden. 

Diese Begrenzung auf eine bestimme lokale Umgebung kann entweder in die Bildung der Kovarianz-Funktion oder direkt in die Deskriptoren implementiert werden.




\subsection{Reduzierung der Trainingsets auf repräsentative Sets, ohne viel Mathematik}
Die Berechnung der Kovarianz Matrix und deren Jacobi-Matrizen ist aufwendig und kann vereinfacht werden. Im Bereich des \textit{Maschinellen Lernens (ML)} ist der Praktik der \textit{Ausdünnung} (engl.: Sparsification) verbreitet. Die Anzahl der Atom-Konfigurationen und die dazugehörigen lokalen Energien werden verkleinert,  die Trainingsdaten werden also minimiert. Dazu muss, beispielsweise durch Zufallsprinzip oder anderen ML-Methoden, eine repräsentative Menge an Atom-Konfigurationen gewählt. Um Genauigkeit zu gewährleisten werden die lokalen Energien der Trainingsdaten zu einer kleineren Menge an Punkten linear-kombiniert. Die Berechnung der lokalen atomaren Energien unter Verwendung von Ausdünnung im Folgenden: \footcite[1054]{GAP-intro}\footcite[3]{GAP-2009}

$$TODO: 2.3 aus Anhang von Gap-2009 plus Erklärung aller Terme gemäß, 2. aus Anhang. Viele Dicke Formeln hier Bruder.$$




\subsection{Beschreibung der Deskriptoren - Berücksichtigen der Invarianzen -Bispectrum}
Deskriptoren beschreiben die lokale Anordnung von Atomen. Sie formen die Geometrie und durch sie wird die lokale Energie determiniert. Haben Deskriptoren zweier Konfigurationen ähnliche Werte, dann haben sie auch ähnliche Energie. Liegt eine spezifische atomare Anordnung vor, also Deskriptoren mit bestimmten Werten, dann wird mit dieser atomaren Anordnung, unter Verwendung quantenmechanischer Methoden (beispielsweise der DFT), die zugehörige Energie berechnet. Diese Paarung, also Geometrie und Energie sind die Daten, mit denen der Gauß´sche Approximierungs Prozess trainiert wird. 

Die einfachsten Deskriptoren sind inter-atomare Abstände und Winkel. Für ein Zwei-atomiges Molekül dient der skalare Abstand als Deskriptor. Ein drei-atomiges Molekül benötigt als Deskriptor zumindest drei inter-atomare Abstände oder zumindest einen inter-atomaren Winkel. Die Komplexität nimmt mit der Größe der Systeme zu. 

Die Energie eines atomaren Umfelds besitzt bestimmte Invarianzen. Sie ist invariant gegenüber Permutation, Translation und Rotation der Atome. Diese Symmetrien müssen also in der Erstellung des Modells berücksichtigt werden. Die Invarianz des Modells ist nicht automatisch gegeben und muss entweder grundlegend in die Deskriptoren eingebaut, oder die Kovarianz-Matrix im Weiteren modifiziert werden. 



Die erste Möglichkeit ist die Implementierung auf Deskriptor-Ebene. Deskriptoren im kartesischen Raum berücksichtigen die notwendigen Symmetrien ebenso wenig wie dreidimensionale sphärische Koordinaten. Eine Möglichkeit um eine genau Beschreibung des Systems, unter Berücksichtigung aller Invarianzen zu gewährleisten ist die Projektion der atomaren Dichte auf eine Kugel im 4-dimensionalen Raum. \footcite[3]{GAP-2009}

$$ Projektion von 3d Koordinaten auf 4d sphäre, --> bispectrum. eventuell neue Quelle$$

Die zweite Möglichkeit, Invarianzen der Energie zu berücksichtigen, ist eine Modifikation der Kovarianz-Matrix. Bei der Bildung des Kernels werden entsprechend Rotations-, Translations-, oder Permutationsoperatoren auf die Basis-Funktionen angewandt. Jeder Satz an Basisfunktionen induziert eine Kovarianzmatrix. Die Basisfunktionen nehmen die Deskriptoren als Argumente. Das invariante Kernel muss normalisiert werden um die Skalierung der Ergebnisse nicht zu verändern. Um Invarianz gegenüber Rotation zu gewährleisten ist die Anwendung eines Rotations-Operators beim berechnen der Kovarianz-Matrix notwendig: \footcite[1055]{GAP-intro}


$$ GAP-intro Formel 40 und 41. Anwendung von R^ um Rotationsinvarianz zu gewährleisten, erklären von \rho. $$



\subsection{SOAP, als tatsächlich verwendete Basis, die als Argumente die Atompositionen nimmt}
VIELLEICHT, weil möglicherweise werden andere Deskriptoren verwendet in der Arbeit. 

\section{Verwendete Software, QUIP, nur kurzer Text Absatz um zu erklären, dass die beschriebenen Konzepte in QUIP umgesetzt sind}




